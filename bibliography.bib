@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{strips,
    title = {Strips: A new approach to the application of theorem proving to problem solving},
    journal = {Artificial Intelligence},
    volume = {2},
    number = {3},
    pages = {189-208},
    year = {1971},
    issn = {0004-3702},
    doi = {https://doi.org/10.1016/0004-3702(71)90010-5},
    url = {https://www.sciencedirect.com/science/article/pii/0004370271900105},
    author = {Richard E. Fikes and Nils J. Nilsson}
}

@misc{openAIwebsite,
    author    = "OpenAI",
    title     = "AI research and deployment company",
    url       = "https://openai.com/"
}

@misc{gymwebsite,
    author    = "OpenAI",
    title     = "Gym, Reinforcement Learning toolkit",
    url       = "https://gym.openai.com/"
}

@article{dietterich2000,
    author = {Dietterich, Thomas G.},
    title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
    year = {2000},
    issue_date = {August 2000},
    publisher = {AI Access Foundation},
    address = {El Segundo, CA, USA},
    journal = {Journal of Artificial Intelligence Research},
    volume = {13},
    number = {1},
    issn = {1076-9757}
}

@Inbook{Hengst2010,
    author="Hengst, Bernhard",
    title="Hierarchical Reinforcement Learning",
    bookTitle="Encyclopedia of Machine Learning",
    year="2010",
    publisher="Springer US",
    address="Boston, MA",
    pages="495--502",
    isbn="978-0-387-30164-8",
    doi="10.1007/978-0-387-30164-8_363",
    url="https://doi.org/10.1007/978-0-387-30164-8_363"
}

@misc{Rshaping,
    author    = "Andrew Y Ng, Daishi Harada and Stuart Russell",
    title     = "Policy invariance under reward transformations: Theory and application to reward shaping",
    year = "1999"
}


@Inbook{RS,
    author="Wiewiora E.",
    title="Hierarchical Reinforcement Learning",
    bookTitle="Encyclopedia of Machine Learning",
    year="2011",
    publisher="Springer US",
    address="Boston, MA",
    doi="10.1007/978-0-387-30164-8_731",
    url="https://doi.org/10.1007/978-0-387-30164-8_731"
}

@inproceedings{dejack,
    author = {De Giacomo, Giuseppe and Vardi, Moshe Y.},
    title = {Linear Temporal Logic and Linear Dynamic Logic on Finite Traces},
    year = {2013},
    isbn = {9781577356332},
    publisher = {AAAI Press},
    abstract = {In this paper we look into the assumption of interpreting LTL over finite traces.
    In particular we show that LTLf, i.e., LTL under this assumption, is less expressive
    than what might appear at first sight, and that at essentially no computational cost
    one can make a significant increase in expressiveness while maintaining the same intuitiveness
    of LTLf. Indeed, we propose a logic, LDLf for Linear Dynamic Logic over finite traces,
    which borrows the syntax from Propositional Dynamic Logic (PDL), but is interpreted
    over finite traces. Satisfiability, validity and logical implication (as well as model
    checking) for LTLf. are PSPACE-complete as for LTLf. (and LTL).},
    booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
    pages = {854–860},
    numpages = {7},
    location = {Beijing, China},
    series = {IJCAI '13}
}

@misc{
    logaut,
    title={Logaut (v0.1.1)},
    url={https://github.com/whitemech/logaut},
    journal={github.com},
    author={Favorito, Marco},
    year={2021}
}

@misc{
    temprl,
    title={TempRL (v0.3.0)},
    url={https://github.com/whitemech/temprl},
    journal={github.com},
    author={Favorito, Marco},
    year={2021}
}

@misc{
    lydia,
    title={Lydia (v0.1.2)},
    url={https://github.com/whitemech/lydia},
    journal={github.com},
    author={Favorito, Marco and Fuggitti, Francesco},
    year={2021}
}

@Article{         harris2020array,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

﻿@Article{Koenig1996,
author={Koenig, Sven
and Simmons, Reid G.},
title={The effect of representation and knowledge on goal-directed exploration with reinforcement-learning algorithms},
journal={Machine Learning},
year={1996},
month={Mar},
day={01},
volume={22},
number={1},
pages={227-250},
abstract={We analyze the complexity of on-line reinforcement-learning algorithms applied to goal-directed exploration tasks. Previous work had concluded that, even in deterministic state spaces, initially uninformed reinforcement learning was at least exponential for such problems, or that it was of polynomial worst-case time-complexity only if the learning methods were augmented. We prove that, to the contrary, the algorithms are tractable with only a simple change in the reward structure ("penalizing the agent for action executions") or in the initialization of the values that they maintain. In particular, we provide tight complexity bounds for both Watkins' Q-learning and Heger's Q-hat-learning and show how their complexity depends on properties of the state spaces. We also demonstrate how one can decrease the complexity even further by either learning action models or utilizing prior knowledge of the topology of the state spaces. Our results provide guidance for empirical reinforcement-learning researchers on how to distinguish hard reinforcement-learning problems from easy ones and how to represent them in a way that allows them to be solved efficiently.},
issn={1573-0565},
doi={10.1007/BF00114729},
url={https://doi.org/10.1007/BF00114729}
}

@misc{degiacomo2019foundations,
      title={Foundations for Restraining Bolts: Reinforcement Learning with LTLf/LDLf restraining specifications}, 
      author={Giuseppe De Giacomo and Luca Iocchi and Marco Favorito and Fabio Patrizi},
      year={2019},
      eprint={1807.06333},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{ijcai2019-840,
  title     = {LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning},
  author    = {Camacho, Alberto and Toro Icarte, Rodrigo and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {6065--6073},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/840},
  url       = {https://doi.org/10.24963/ijcai.2019/840},
}

@inproceedings{ltl,
    author = {A. Pnueli},
    booktitle = {2013 IEEE 54th Annual Symposium on Foundations of Computer Science},
    title = {The temporal logic of programs},
    year = {1977},
    volume = {},
    issn = {0272-5428},
    pages = {46-57},
    keywords = {},
    doi = {10.1109/SFCS.1977.32},
    url = {https://doi.ieeecomputersociety.org/10.1109/SFCS.1977.32},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {oct}
}